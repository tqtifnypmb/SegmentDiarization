import torch
from dataclasses import dataclass
from torch import Tensor
from typing import Dict
from .model import SegmentDecoder

@dataclass
class ModelDimensions:
    n_audio_state: int
    n_audio_head: int
    n_audio_layer: int
    n_ctx: int
    n_speakers: int

class SegmentDiarization:
    def __init__(self, ckpt_path: str) -> None:
        checkpoint = torch.load(ckpt_path)
        self.dmis = checkpoint['dims']
        self.n_speakers = self.dmis.n_speakers + 1  # plus 1 for silence - '0'

        self.model = SegmentDecoder(n_ctx=self.dmis.n_ctx, n_state=self.dmis.n_audio_state, n_head=self.dmis.n_audio_head, n_layer=self.dmis.n_audio_layer)
        self.model.load_state_dict(checkpoint['model'])

    def decode(
            self, 
            features: Tensor, 
            sentence_threshold: float = 1.5, 
            speaker_switch_threshold: float = 1, 
            return_raw_results = False
        ) -> Dict[int, list[Dict]]:
        """
    Decode the audio features generated by the Whisper's tiny encoder into segments segmented by speakers.

    Parameters
    ----------
    features: Tensor
        Audio features generated by the Whisper's tiny encoder

    sentence_threshold: float
        Segments with a time interval shorter than the sentence_threshold are considered to be part of the same sentence

    speaker_switch_threshold: float
        When there is a speaker switch, if the duration of speech is less than the speaker_switch_threshold, the original speaker is retained

    return_raw_results: bool
        Skip post processing, return raw results

    Returns
    -------
    A dictionary containing the speaker ("speaker") and segment start time ("start"), and
    end time ("end"). If audio is silent, return None.
    """
        
        logits, _ = self.model(features)
        logits = torch.softmax(logits, -1)
        logits = torch.argmax(logits, -1)
        
        batch_segments: Dict[int, list[Dict]] = {}
        def append_segment(audio_idx: int, segment: Dict):
            if audio_idx in batch_segments:
                batch_segments[audio_idx].append(segment)
            else:
                batch_segments[audio_idx] = [segment]
            

        speaker_changed = (logits[:, :-1] != logits[:, 1:]).nonzero()
        batch_beg_timestep = torch.zeros((features.shape[0],)).to(torch.long)
        for idx in range(speaker_changed.shape[0]):
            audio_idx = speaker_changed[idx][0].item()
            end_timestep = speaker_changed[idx][1].item() + 1
            beg_timestap = batch_beg_timestep[audio_idx].item()

            assert(logits[audio_idx, beg_timestap: end_timestep].allclose(logits[audio_idx, beg_timestap]))

            speaker_id = logits[audio_idx, beg_timestap].item()
            if speaker_id == 0:
                # silent
                batch_beg_timestep[audio_idx] = end_timestep
                continue

            beg_time = beg_timestap * 20 / 1000
            end_time = end_timestep * 20 / 1000
            ith_speaker = int(speaker_id / self.n_speakers) + 1
            overlap_speaker = speaker_id % self.n_speakers - 1

            append_segment(audio_idx, { "speaker": ith_speaker, "start": beg_time, "end": end_time, "overlap_speaker": overlap_speaker })
            batch_beg_timestep[audio_idx] = end_timestep

            print(f"speaker: {ith_speaker}, beg: {beg_time}, end: {end_time}, overlap_speaker: {overlap_speaker}")

        for audio_idx in range(batch_beg_timestep.shape[-1]):
            beg_timestap = batch_beg_timestep[audio_idx].item()
            if beg_timestap >= 1500:
                continue

            end_timestep = 1500
            assert(logits[audio_idx, beg_timestap: end_timestep].allclose(logits[audio_idx, beg_timestap]))

            speaker_id = logits[audio_idx, beg_timestap].item()
            if speaker_id == 0:
                # silent
                continue

            beg_time = beg_timestap * 20 / 1000
            end_time = end_timestep * 20 / 1000
            ith_speaker = int(speaker_id / self.n_speakers) + 1
            overlap_speaker = speaker_id % self.n_speakers - 1
            append_segment(audio_idx, { "speaker": ith_speaker, "start": beg_time, "end": end_time, "overlap_speaker": overlap_speaker })

            print(f"speaker: {ith_speaker}, beg: {beg_time}, end: {end_time}, overlap_speaker: {overlap_speaker}")

        if return_raw_results:
            return batch_segments
        
        return self._post_process(batch_segments, sentence_threshold, speaker_switch_threshold)

    def _post_process(self, batch_segments: Dict[int, list[Dict]], sentence_threshold: float, speaker_switch_threshold: float) -> Dict[int, list[Dict]]:
        results: Dict[int, list[Dict]] = {}

        def merge(segments_to_merge: Dict[int, list[Dict]]):
            results: Dict[int, list[Dict]] = {}
            for audio_idx, segments in segments_to_merge.items():
                merged_segments = []

                first_segment = segments[0]
                last_segment = None
                idx = 0
                while idx < len(segments):
                    current_segment = segments[idx]

                    is_same_speaker = current_segment['speaker'] == first_segment['speaker']

                    if last_segment is not None:
                        close_enough = current_segment['start'] - last_segment['end'] < sentence_threshold
                    else:
                        close_enough = current_segment['start'] - first_segment['end'] < sentence_threshold

                    if is_same_speaker and close_enough:
                        last_segment = current_segment
                    else:
                        if last_segment is not None:
                            first_segment["end"] = last_segment['end']
                        merged_segments.append(first_segment)
                        
                        last_segment = None
                        first_segment = current_segment

                    idx += 1
                
                if first_segment is not None:
                    if last_segment is not None:
                        first_segment["end"] = last_segment['end']
                    merged_segments.append(first_segment)

                results[audio_idx] = merged_segments
                
            return results

        # 1. merge short segment with same speaker
        merged_batch_segments = merge(batch_segments)   

        # 2. remove rapid speaker switches 
        for audio_idx, merged_segments in merged_batch_segments.items():
            for idx, segment in enumerate(merged_segments):
                duration = segment['end'] - segment['start']
                if duration < speaker_switch_threshold:
                    if idx > 0:
                        prev_segment = merged_segments[idx - 1]

                        is_speaker_changed = prev_segment['speaker'] != segment['speaker'] and prev_segment['speaker'] != 0
                        close_enough = segment['start'] - prev_segment['end'] < sentence_threshold
                        if is_speaker_changed and close_enough:
                            prev_segment['end'] = segment['end']

                            # mark as removed
                            segment['speaker'] = 0
                            continue

                    # if idx + 1 < len(merged_segments):
                    #     next_segment = merged_segments[idx + 1]

                    #     is_speaker_changed = next_segment['speaker'] != segment['speaker']
                    #     close_enough = next_segment['start'] - segment['end'] < sentence_threshold
                    #     if is_speaker_changed and close_enough:
                    #         next_segment['start'] = segment['start']

                    #         # mark as removed
                    #         segment['speaker'] = 0
                    #         continue

            results[audio_idx] = [segment for segment in merged_segments if segment['speaker'] > 0]

        final_results = merge(results)
        return final_results